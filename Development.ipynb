{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76531089-5a3a-4941-9137-1381f9c1d33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch #PyTorch: https://pytorch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528ef5fb-f2a0-4298-bb08-180dfced31d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open(file='files/shakespeare.txt', mode='r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b41828-5782-450c-9d79-110de4be3925",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset (complete works of Shakespearre) in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset (complete works of Shakespearre) in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81de949a-c877-4a3c-b297-433206bdbdc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250]) #1st 250 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e470a8c9-bc97-4ce3-bf72-fef930e4e9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The characters are: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz .\n",
      "With  65  words in the Shakespearean vocabulary.\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text))) #sort(create list of(the set of all characters in text))\n",
    "vocabulary_size = len(characters) \n",
    "print(\"The characters are:\",''.join(characters),\".\\nWith \",vocabulary_size,\" words in the Shakespearean vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad8e4d-cdf8-4ef5-9494-68a2b1eed3ff",
   "metadata": {},
   "source": [
    "Develop strategy to tokenize raw text, which is a means representing the entire possible vocabulary through a series of integers.  Since this is a character-level language model, we will need to tokenize the individual characters, of which there are 65. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578895ff-18da-475d-ab8e-88639c25f411",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr><br>\n",
    "\n",
    "### <center>Tokenizing: Encoding & Decoding</center>\n",
    "\n",
    "Sub-word vs Character Level - tradeoff\n",
    "\n",
    "Many ways to accomplish this ...\n",
    "\n",
    "- Google uses [SentencePiece](https://github.com/google/sentencepiece) - sub-word level tokenizer\n",
    "- OpenAI (ChatGPT) uses [tiktoken](https://github.com/openai/tiktoken) - BPE (Byte-Pair Encoding) tokenizer\n",
    "- Will build a basic one below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1bfb5f-b093-4a9b-85e2-05a9648e7c7e",
   "metadata": {},
   "source": [
    "In Python, a dictionary (or lookup table) can be initialized via `d={}`, & `enumerate()` can be used to iterate over a list.<br><br>\n",
    "So, if we initialize a dictionary using the folowing:<br>\n",
    "`{ character:integer for integer,character in enumerate(characters) }`,<br>\n",
    "We declare a lookup table with Key-Value pairs being set to `character:integer`, such that each of the 65 (0-64) characters in the sorted `list` are mapped to integers; characters -> integers.<br> We can repeat the process & use `integer:character` in the initialization for create a second mapping from integers --> characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bdb18b-d467-46c7-b44a-645c5d3ae12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "character_to_integer_dictionary = { character:integer for integer,character in enumerate(characters)}\n",
    "integer_to_character_dictionary = { integer:character for integer,character in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0a86cb-156b-438e-81a3-f4ee96f3ad81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('\\n', 0), (' ', 1), ('!', 2), ('$', 3), ('&', 4), (\"'\", 5), (',', 6), ('-', 7), ('.', 8), ('3', 9), (':', 10), (';', 11), ('?', 12), ('A', 13), ('B', 14), ('C', 15), ('D', 16), ('E', 17), ('F', 18), ('G', 19), ('H', 20), ('I', 21), ('J', 22), ('K', 23), ('L', 24), ('M', 25), ('N', 26), ('O', 27), ('P', 28), ('Q', 29), ('R', 30), ('S', 31), ('T', 32), ('U', 33), ('V', 34), ('W', 35), ('X', 36), ('Y', 37), ('Z', 38), ('a', 39), ('b', 40), ('c', 41), ('d', 42), ('e', 43), ('f', 44), ('g', 45), ('h', 46), ('i', 47), ('j', 48), ('k', 49), ('l', 50), ('m', 51), ('n', 52), ('o', 53), ('p', 54), ('q', 55), ('r', 56), ('s', 57), ('t', 58), ('u', 59), ('v', 60), ('w', 61), ('x', 62), ('y', 63), ('z', 64)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_to_integer_dictionary.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c8b632-662e-423c-8051-6dec016c350c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, '\\n'), (1, ' '), (2, '!'), (3, '$'), (4, '&'), (5, \"'\"), (6, ','), (7, '-'), (8, '.'), (9, '3'), (10, ':'), (11, ';'), (12, '?'), (13, 'A'), (14, 'B'), (15, 'C'), (16, 'D'), (17, 'E'), (18, 'F'), (19, 'G'), (20, 'H'), (21, 'I'), (22, 'J'), (23, 'K'), (24, 'L'), (25, 'M'), (26, 'N'), (27, 'O'), (28, 'P'), (29, 'Q'), (30, 'R'), (31, 'S'), (32, 'T'), (33, 'U'), (34, 'V'), (35, 'W'), (36, 'X'), (37, 'Y'), (38, 'Z'), (39, 'a'), (40, 'b'), (41, 'c'), (42, 'd'), (43, 'e'), (44, 'f'), (45, 'g'), (46, 'h'), (47, 'i'), (48, 'j'), (49, 'k'), (50, 'l'), (51, 'm'), (52, 'n'), (53, 'o'), (54, 'p'), (55, 'q'), (56, 'r'), (57, 's'), (58, 't'), (59, 'u'), (60, 'v'), (61, 'w'), (62, 'x'), (63, 'y'), (64, 'z')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_to_character_dictionary.items() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380118de-1379-47a4-8c44-f0127edd58d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encode = lambda string: [character_to_integer_dictionary[character] for character in string] #input string --> output integer mapping\n",
    "decode = lambda list: ''.join([integer_to_character_dictionary[integer] for integer in list]) #input list of integers --> output original characters from mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f529ea5-355e-475f-a9d6-e89a8ac9b4a7",
   "metadata": {},
   "source": [
    "## <center> Tokenizing `\"Hello World!\"` </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b53f52-7eb9-4041-ad25-46a733e2be64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phrase = \"hello world!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e5dc5be-868c-4da0-ab81-1c07a02bca49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Message:  [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2] --> Decoded Message:  hello world!\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenizer built for our purposes; remember that our dictionaries have 65 elements\n",
    "encoded_tmp = encode(phrase)\n",
    "print(\"Encoded Message: \",encoded_tmp,\"--> Decoded Message: \", decode(encoded_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e0ebf-e677-4b7c-93be-49bc29065dd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let us use a sub-word encoder, like **tiktoken** on the same phrase, not that encoding contains fewer characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67f0c33b-38be-450e-8197-f89c4daf0707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Message:  [31373, 995, 0] --> Decoded Message:  hello world!\n"
     ]
    }
   ],
   "source": [
    "tt_Encoded = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "print(\"Encoded Message: \",tt_Encoded.encode(phrase),\"--> Decoded Message: \", tt_Encoded.decode(tt_Encoded.encode(phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751e9f1-5be2-45e4-a6ad-0b89ee9d6bdc",
   "metadata": {},
   "source": [
    "<h2 align=center> Tensors</h2>\n",
    "\n",
    "In Machine Learning, **parrellization** is extensively used, because (mathematical) operations can be applied to multiple elements simoultaneously, as opposed to a sequential or stepwise fashion.<br>Similar to being asynchronous.<br><br>Tensors are just arrays of values which are utilized to make parrelization possible.  Tensors can be:\n",
    "- Rank 0: Scalar (Magnitude) - 0x1\n",
    "- Rank 1: Vector (0+Direction) - 1x1\n",
    "- Rank 2: Dyad (1+Direction) - 2x2\n",
    "- Rand 3: Triad (2+Direction) - 3x3\n",
    "\n",
    "<h4 align=center>Utilizing the simple encode() & decode() functions we created, let us create a tensor which wraps the encoded text called 'data'</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63122fc1-f56a-47ab-9a08-b9d04a122779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#text contains the tiny shakespeare which was read in earlier; \n",
    "#will encode the text & wrap it into a tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7591512-74b9-4804-96fe-6d5ac6e017cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of elements from the encoded text is 1,115,394 encoded & wrapped into a tensor with the data type is ' torch.int64 '.\n",
      "The first 100 elements are:  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59]) \n",
      "these elements will correspond exactly with the 1st 100 characters from the original tiny shakespeare text\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print('The number of elements from the encoded text is {:,}'.format(data.shape[0]), 'encoded & wrapped into a tensor with the data type is \\'',data.dtype,'\\'.\\nThe first 100 elements are: ',data[:100],'\\nthese elements will correspond exactly with the 1st 100 characters from the original tiny shakespeare text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8abe2d20-194f-48c0-9c46-a9f89110cd27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100]) # = decode(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ef329-af5a-4561-bec2-9fe3489265e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 align=center>Split Data into Training & Validation Sets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1239e949-a00c-4a19-b54b-bfa411454df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # n = 90% of the # of elements available\n",
    "training_data = data[:n] # slice 90% into training \n",
    "validation_data = data[n:] # slice 10% into validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7bce0-acbc-4ae9-88f7-6ec929720e05",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>We now need to feed the text into the transformer, but cannot do so at once (prohibitive), so instead we utilize random sampling of random chunks.  A 'block size' refers to the size of each of these chunks</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b772cc8d-ffd4-4ee0-b48e-35798c6b5ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc44c38-dc3d-4858-9662-dcc202211362",
   "metadata": {},
   "source": [
    "To see this in action, consider the next block of code, where as the number of elements are added, you see the output is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c062c6c-9fef-4b51-9b51-5acd6f71ba56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([18]), the Target is 47\n",
      "When the input is tensor([18, 47]), the Target is 56\n",
      "When the input is tensor([18, 47, 56]), the Target is 57\n",
      "When the input is tensor([18, 47, 56, 57]), the Target is 58\n",
      "When the input is tensor([18, 47, 56, 57, 58]), the Target is 1\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1]), the Target is 15\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15]), the Target is 47\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the Target is 58\n"
     ]
    }
   ],
   "source": [
    "#W/ a block size of 8, the transformer is being trained on each of the following, i.e. a context of size 1, through a context of size 8;\n",
    "#This gives the transformer all-sized contexts\n",
    "x = training_data[:block_size]\n",
    "y = training_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When the input is {context}, the Target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb845723-ffad-4cea-ad75-543527917c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[:block_size+1] #adding +1 will give the transfomer 8 unique examples of character associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763f3b4-ab25-42ee-83be-a9b4e63f5e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedd248-c71f-4fff-ac7e-83f0d8b955f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pythonomy",
   "language": "python",
   "name": "pythonomy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
